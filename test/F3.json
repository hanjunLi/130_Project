{
  "metadata": {
    "title": "Hierarchical graph embedding in vector space by graph pyramid",
    "authors": "Seyedeh Fatemeh Mousavi, Mehran Safayani, Abdolreza Mirzaei, Hoda Bahonar"
  },
  "tracks": {
    "1": {
      "id": "title and authors",
      "content": "Hierarchical graph embedding in vector space by graph pyramid by Seyedeh Fatemeh Mousavi, Mehran Safayani, Abdolreza Mirzaei, Hoda Bahonar"
    },
    "2": {
      "id": "Abstract",
      "content": "Loss of information is the major challenge in graph embedding in vector space which reduces the impact of representational power of graphs in pattern recognition tasks. The objective of this article is to present a hierarchical framework which can decrease this loss in a reasonable computational time. Inspired by multi-resolution ideas in image processing, a graph pyramid is formed based on a selected graph summarization algorithm which can provide the required information for classiﬁcation. All the pyramid levels or some of them are embedded into a vector through an available embedding method which constructs an informative description containing both local and global features. The experiments are conducted on graphs with numerical and categorical attributes. In the numerical case, a proposed summarization algorithm is applied while in the categorical case, k-SNAP graph summarization is applied. The results indicate that this new framework is efﬁcient in terms of accuracy and time consumption in the context of classiﬁcation problems. It is observed that this improvement is achieved regardless of selected embedding techniques."
    },
    "3": {
      "id": "1. Introduction",
      "content": "The superior representational power of graphs introduces these data structures as popular replacements for vectors in many pattern recognition applications. Lack of algorithmic tools is one of the main drawbacks of the graph structure. To overcome this drawback, graph embedding in vector space [1–7] provides the opportunity to beneﬁt from the rich repository of vector based methods which are the results of many years of research and experience. However, the problem of ﬁnding appropriate vector representations for graphs is not an easy task. Two issues concern embedding methods: ﬁrst, embedding procedures should not include costly operations and second, extracted features should preserve graph information, the structural details in particular as much as possible. These opposite requirements become more remarkable in larger and more complex graphs. In fact, with growing size of a graph, the cost of extracting useful information from all its components increases. Moreover, the augmented structural complexity raises the possibility of noise and distortion in structure and content of the original graph and this phenomenon makes the feature extraction more critical. As a result, in order to obtain a vectorial representation of a graph, a compromise between the embedding time and the ability of preserving information must be established, with respect to application. To accomplish this objective, a hierarchical framework is introduced in this article. \n Multi-resolution processing is employed successfully in computer vision and image processing algorithms. This method tries to simulate the main characteristics of human visual perception [8]. For this purpose, the applied data structure should be able to represent both the distributed local information of the raw data and the hidden global information within. This structure is named the hierarchical architecture [9] the general scheme of which is shown in Fig. 1. It is observed that this hierarchical structures can be applied to efﬁciently extract the global information (image interpretations) from the local ones (array of pixel intensities) in a recursive manner. This structure provides an appropriate manner in order to cope with the limitations of memory and time and is considered as a bridge between local information and global interpretation. \n In the notion of the hierarchical structure, the graphs are very similar to the images, that is, although the local processing through checking labels and adjacency relations can provide useful information about the content of the graph, accurate understanding of the global graph information requires the processing of substructures in different scales. This processing may be more important in matching and classifying large graphs, since the effective features cannot be detected merely with visual inspection and they can occur in every scale. Depending on the application, the similarities on one or all the scales may be of interest. Therefore, the processing at different scales is necessary in the graph domain. Accordingly, transferring the concept of image pyramid to the graph domain can be beneﬁcial. \n Based on this idea, this article introduces a framework named pyramidal graph embedding (PyrGE) the objective of which is to provide easy access to a variety of information of the graph hierarchy and to apply them in reducing the missing information during embedding. It is expected that this framework increases the classiﬁcation accuracy by reducing the complexity of managing large graph data. In this approach, a graph summarization algorithm [10,11] is employed as an operator for graph data localization in different scales. The graphs of different levels are embedded into the vectors using the single scale embedding methods and the obtained vectors are combined into a single vector. \n In brief, the main contribution of this article is to apply an extracted hierarchical graph set rather than just applying the original graph for embedding in order to extend the representational power of the obtained embedded graph. The proposed framework is generic enough to handle different types of graphs provided that proper graph summarization and embedding methods are adapted to the type of graphs. In order to show this adaption, the experiments are run on three datasets, the ﬁrst two datasets contain numerical node labels and last one contains categorical node label. For the numerical datasets, a new summarization procedure appropriate for this kind of graphs is proposed while for the categorical dataset, an existing summarization method, k-SNAP [11] is applied. The main achievement of this assessment is that regardless of the selected embedding method for each level, embedding procedure through this proposed hierarchical architecture improves the classiﬁcation accuracy in all experiments in a signiﬁcant manner. \n The outline of this paper is organized as follows. The literature review is presented in Section 2; the primary concepts are discussed in Section 3; the details on how graph pyramid can be applied to enhance the existing embedding methods is expressed in Section 4; the design of several scenarios with respect to this framework is presented in Section 5; the concluding remarks and future works complete this article in Section 6."
    },
    "4": {
      "id": "2. Literature review‌",
      "content": ""
    },
    "5": {
      "id": "2.1. Graph embedding",
      "content": "The embedding methods can be divided into three groups based on their feature extraction approach. The ﬁrst group is inspired by dissimilarity representations proposed in [12]. Riesen and Bunke [1] present the vectorial description of a graph by its distances to a number of prototype graphs. The second group is based on the frequencies of appearance of some speciﬁc knowledge-dependent substructures, capturing topology information and content of graph through its labels. This group is known as graph probing based methods [13]. For example, in [14] the vector is built by counting the occurrences of non-isomorphic model graphs included in the target graph. The recent approach of Gibert et al. [2] is based on different statistics on the node labels and the edge relations among them. Luqman et al. [3] consider the graph information in several levels of topology, structure and attributes. The third group, the spectral graph embedding, is a prominent group based on feature extraction from eigen-decomposition of adjacency or Laplacian matrices [15,4]. For example, Ren et al. [5] extracted cycle frequencies as the graph features through the Ihara Zeta function. Aziz et al. [6] applied backtrackless walks to capture the path and cycle information by avoiding tottering. \n Each one of these groups has some drawbacks, which become more apparent in the case of large graphs. Dissimilarity-based methods (group 1) are able to handle arbitrary graphs and cope with structural distortions due to the use of graph edit distance as the basic dissimilarity measure. However, its computational cost is a challenge in dealing with large graphs. Probing based methods (group 2) are capable of using speciﬁc domain knowledge by ﬁnding substructures, but this procedure has high computational cost of subgraph isomorphism. Newer probing-based methods rely more on labels and binary relations and somewhat ignore the complex substructures of relatively large graphs. The spectral methods (group 3) provide intriguing and meaningful properties of a graph with polynomial time complexity, while they have the built-in problem of sensitivity to noise and are restricted to graphs with strongly limited label alphabets."
    },
    "6": {
      "id": "2.2. Pyramidal models",
      "content": "A wide range of applications such as image segmentation [16], region description and detection [17] make use of pyramid as a useful structure. The image pyramid is a collection of reduced resolution images, which are computed by an iterative procedure [18]. The pyramids can be divided into two categories: the regular pyramids [19] which have some problems [20] due to their ﬁxed decimation process and the irregular pyramids [20] which solve these problems through dynamic graph structures, but their handling is more time consuming due to not speciﬁed height of pyramid and size of each level. \n Pyramidal models can provide an approach to simulate the mental mechanisms as well as the transformation process from perception to reasoning. This approach can offer sub-optimal algorithms which have the linear computational complexities similar to the mental processes' [21]. Pizlo et al. [22] adopt a multilevel pyramidal model for modeling the size transformation and size perception of human. Haxhimusa et al. [23] applied this approach in order to ﬁnd near-optimal tour in the traveling salesman problem. They formed a graph where cities were its nodes and the neighboring cities' correlations were its edges. The graph pyramid was built based on the minimum spanning tree principle. The nodes of every level were grouped into trees with a predeﬁned height and applied as the nodes of the next level."
    },
    "7": {
      "id": "2.3. Graph summarization",
      "content": "Graph summarization methods have a major contribution in resolution reduction of the graphs. The researchers in different disciplines apply different approaches to extract graph summaries. In physics and biology, statistical measures [24] such as degree distribution are considered as the summaries of the graph. In pattern extraction, the summaries are a set of frequent subgraphs [25]. In community detection, resulting partitions from a node partitioning algorithm are applied as the summaries of the graph [26]. In this study, the focus is on the methods the output of which is the graph. \n These graph summarization algorithms can be classiﬁed into the structural and the semantical approaches. The structural methods summarize graphs according to the edges' structure and common neighborhoods, for example, the authors of [10] proposed the S-node representation the super-nodes and super-edges of which are the pointers to the partitions of nodes and edges of the given unlabeled web graph, respectively. Navlakha et al. [27] enhanced this summary representation with a set of edge-corrections necessary to regenerate the original graph. In the semantical summarization, attributes and statistics of the nodes and edges are applied, for example, Tian et al. [11] introduced the k-SNAP operation which produces a summary graph of size k by grouping nodes based on preselected node attributes and relationships. \n A node clustering based algorithm can be of assistance for summarizing the numerical attributed graphs. Almost all node clustering algorithms apply the afﬁnity matrix in modeling the similarities between nodes. With respect to the processing method of this matrix, the node clustering approaches can be divided into: optimization of graph clustering objectives [28], spectral methods [29], kernel-based methods [30] and multi-level methods [28,31]."
    },
    "8": {
      "id": "3. Concepts and notations‌",
      "content": ""
    },
    "9": {
      "id": "3.1. Graph",
      "content": "Deﬁnition 1 (Graph). Let [math] and [math] be a ﬁnite or inﬁnite label sets for nodes and edges, respectively. A graph g is a four-tuple [math], where V is the ﬁnite set of nodes, [math] is a set of edges, [math] is the node labeling function and [math] is the edge labeling function. \n The graph domain g is the set of all graphs over the label alphabets [math] and [math]. The values [math] and [math] are the graph order and size, respectively."
    },
    "10": {
      "id": "3.2. Graph edit distance",
      "content": "Graph edit distance is the most ﬂexible error tolerant graph matching method able to deal with arbitrary graphs and has obtained robust results against noise. \n Deﬁnition 2 (Graph Edit Distance). If [math] is the source graph and [math] is the target graph, then the graph edit distance between [math] and [math] is deﬁned as [math] where [math] is the cost of edit operation [math] and [math] is the set of edit paths from [math] to [math]. \n An edit path from [math] to [math] is a sequence of edit operations that convert [math] into [math] [32]. A standard set of edit operations is deﬁned as follows: [math] where, [math] and [math] are the insertion and deletion of node (edge) a, respectively, and [math] is the substitution of the node (edge) a with the node (edge) b. Here, edit distance computation is run by the suboptimal method proposed in [33] which has the polynomial complexity [math] in the number of nodes in two graphs."
    },
    "11": {
      "id": "4. Pyramidal graph embedding",
      "content": "Regardless of applied method, graph embedding in vector space leads to the loss of some graph information due to the lower representational power of the vectors. Moreover, any embedding procedure applied only on a single scale may miss some information at the other scales. Inspired by multi-resolution theory, pyramidal graph embedding seeks to retrieve some of the lost information. In order to accomplish this task, the global information in small scales is localized into the large scales through a summarization algorithm. In this manner, effective global and local features of the given graphs are extracted from different scales simultaneously. \n The overall view of pyramidal graph embedding framework is shown in Fig. 2. As observed, this framework consists of two independent subroutines: (1) the graph pyramid extraction subroutine that takes the base level graphs as the input and returns the graph pyramids as the output by applying an adequate localization operator repeatedly and (2) multi-scale embedding subroutine that uses the graph pyramids as the input and embeds the graphs of different levels into the vector space, following by combining the resulting vectors from each graph through the merging procedure and produces the ﬁnal vectors."
    },
    "12": {
      "id": "4.1. Graph pyramid",
      "content": "Assume the graph [math] and its corresponding pyramid as [math], shown in Fig. 3. There exists a reduced graph of g at every level of [math]. The graph order (here considered as the graph resolution) is reduced with a reduction factor of [math], by moving towards the next upper layer. Therefore, the number of levels is equal to [math]. The base level (zero level) is the original graph with graph order [math] and the top level (Lth level) is the trivial graph with graph order 1. The intermediate level l is [math]., where [math] has [math]. nodes. The graph pyramid limits itself to [math]. reduced resolution approximations from the original graph, because usually the high abstract levels like the two top levels displayed in Fig. 3, are not informative enough; Hence, the graph pyramid is truncated to [math] levels such that the highest abstract level is the Pth level and the most detailed level is the base level. \n As mentioned earlier, the nodes of the graph of level [math] are obtained by applying a reduction procedure on the nodes of level l, that is, a set of nodes in the lower level are grouped into a node in the upper level through a contraction scheme. These vertical relations are indicated with dashed lines in Fig. 3. The horizontal relations are the edges of the reduced graphs which are formed based on the adjacency relations and inter-node similarities. These relations are shown with straight lines in Fig. 3. \n The graph summarizations which allow users to control the resolutions of summaries are suggested as the localization operator for graphs. The summarization algorithm is run on [math] in the lth level and produces the output graph [math]. The graphs of different levels ( [math] to [math]) are obtained through recursive running of this process. Although ﬁnding some speciﬁc substructures may be considered as another option to solve this problem, it has high computational complexity of subgraph isomorphism. Likewise, the spectral embedding methods require fewer computations, while they have difﬁculties in meeting structural distortions. Consequently, graph summarization is an adequate option which can offer a proper view of the given graph with relatively low computational complexity. In the framework, depending upon the kind of graphs, the appropriate summarization procedure should be adopted. The following subsections describe two different graph summarization alternatives which are employed in the experiments."
    },
    "13": {
      "id": "4.1.1. Numerical attributed graph summarization",
      "content": "An appropriate graph summarization algorithm for numerical node attributed graphs is proposed here (Algorithm 1). This algorithm is a node clustering based approach that has ability to deal with numerical attributes and to control the resolution of summary graphs. In step I, the super-nodes are generated by applying a clustering technique (as the contraction scheme) on the nodes of [math] and considered as the nodes of [math]. In step II, the super-edges are inserted according to the number of edges between each pair of super-nodes; In step III, the labeling functions of [math] are deﬁned with respect to the labeling functions of [math]. \n I. Generate super-nodes: Although any node clustering algorithms can be used to generate super-nodes, spectral methods due to its efﬁciency and production of high-quality partitions is of interest when eigenvector computation is feasible. Hence, a simple algorithm based on spectral factorization [34] is used, here. The input of this method is an afﬁnity matrix [math] whose entry [math] encodes the cost associated with cutting the edge between nodes u and v, depending on knowledge and assumptions of the underlying problem. After making A doubly stochastic, the matrix [math] is formed by inserting the eigenvector scorresponding to the k largest eigenvalues of A in its columns. The k-means algorithm is performed on the rows of U as the representation of the graph nodes to get k partitions as the supernodes. \n II. Insert super-edges: In order to make decision about the structure of the summary graph, a thresholding method is adopted. Assume [math] is equal to the number of possible edges between two super-nodes [math] and [math], and [math] is the existing edges between them. If the ratio of [math] to [math] is greater than the speciﬁc threshold [math], it means that those super-nodes have sufﬁcient connection and the edge insertion between them is reasonable. If [math], [math], is the number of edges of a complete bipartite graph between [math] and [math], and the threshold is called δedge. If [math] is the number of edges of a complete graph containing the members of [math], and the threshold is called [math]. The [math] should be greater than [math], provided that there is a tendency to keep the simplicity of the graph. \n III. Labeling nodes and edges: Various labels can be assigned to the nodes and edges depending on the application and the importance of graph reconstruction. The label of super-node [math] can represent some statistical measures like the number of the internal nodes of [math] and the average and the standard deviation of their labels. The label of the super-edge ([math], [math]) can either be [math] or [math]. For the purpose of this article, the average of the node coordinates as the labels of super-nodes would sufﬁce, because the objective here is to obtain a more abstract view of graphs in order to embed them."
    },
    "14": {
      "id": "4.1.2. Categorical attributed graph summarization‌",
      "content": "We suggest an existing algorithm called k-SNAP [11] to sum-marize categorical attributed graphs. Given a graph [math], the goal of k-SNAP is to produce a summary graph where a node grouping of size k, [math], is considered as set of super-nodes. A super-edge is inserted between two super-nodes if there exists at least one edge connecting two nodes of them. k-SNAP tries to ﬁnd a summary that has best quality based on a proposed [math]-measure which assesses number of differences between the resulted summary with the ideal summary. If [math] and the participation ratio of ([math], [math]) is deﬁned as [math]-measure of a grouping [math] is [math] where [math]. \n Finding the optimal summary with the minimum [math]-value is NP-Complete problem, so a heuristic based approach called top-down is used. It begins with a compatible grouping in which all nodes in each group have the same value on preselected attributes. Until the size of grouping is less than k, it iteratively chooses a group [math] which has a maximum value [math] with one of its neighbor groups, and then splits it based on its connections with [math]."
    },
    "15": {
      "id": "4.2. Multi-scale embedding",
      "content": "The last step in pyramidal framework is multi-scale embedding. To obtain the feature vector, a pool of the embedding methods and the subsequent selection strategy could be applied. This selection strategy can be set up in a manner that it picks a single method for all levels, a separate method for each level or a bunch of methods for every level. For example, assume that in an application the graphs are large and noisy; here, high computational methods (e.g. dissimilarity-based) or noise sensitive methods (e.g. spectral) face challenges, and when these graphs move towards the higher levels, the high computational and noise sensitive methods can be adopted due to a reduction in size and error of the graphs. The vector merging procedure combines the resulting vectors of the pyramid levels into a single vector. \n In this article, in order to study the effect of the hierarchical framework in improving the existing embedding methods, a single method is adopted for embedding all levels of the graph pyramid and they are combined simply by concatenating them in a long vector where all level combinations are compared in terms of accuracy and time. Meta-parameters of each level are calculated by a validation procedure with respect to other levels of graph pyramid. According to the above descriptions, the formal deﬁnition of this proposed framework can be stated as follows: \n Deﬁnition 3 (Pyramidal graph embedding). Consider that the graph set [math] and the corresponding graph pyramid set [math]. If the base single-scale embedding method applied on graph set [math] of [math] is [math] where, [math], then the pyramidal graph embedding [math] is deﬁned as follows: [math]. \n In the following experiments, the strength of this pyramidal framework is veriﬁed empirically in order to shed more light on its ability in dealing with graphs of some different domains."
    },
    "16": {
      "id": "5. Experiments‌",
      "content": ""
    },
    "17": {
      "id": "5.1. Graph datasets",
      "content": "To demonstrate the generality of the framework, experiments are performed on three datasets including Columbia Object Image Library (COIL) [35] and the Object DataBanK (ODBK) [36] which are two object detection graph datasets containing numerical labels, and Enzymes dataset [37] with categorical labels consists of the graphs of tertiary structures of proteins. Since the power of this hierarchical framework is more evident in large graphs with a meaningful hierarchy, graphs with appropriate size from these datasets are selected. \n The COIL image dataset consists of 128 images of 100 different objects acquired from 72 equally spaced poses. The ODBK image dataset consists of 450 images of 14 views of 209 3D object models, while only 12 views of them are applied here. In order to extract the underlying graphs of the given images, the corners are detected through Harris corner detector [38] and considered as the graph nodes. The edges are established through Delaunay triangulation between these nodes. The obtained graphs are unweighted and the coordinates of the corner points represent the node labels. For these experiments, 15 and 50 classes, with maximum average number of nodes are selected from COIL and ODBK graphs, respectively. \n The graphs of Enzymes dataset have one categorical type label on their nodes and their edges are free of labels. By selecting the connected graphs with greater graph orders, a graph dataset of 480 Enzymes, 80 graphs for each class is obtained. The characteristics of all datasets are tabulated in Table 1."
    },
    "18": {
      "id": "5.2. Numerical attributed graphs",
      "content": "The objective here is twofold: (I) the proposed summarization method is examined to show how this method can favorably preserve the necessary information to classify the graphs, (II) exhibit the power of this framework in enriching the three embedding methods, each selected from either of groups. The objective II is empirically conﬁrmed through k-Nearest Neighbor (k-NN) and Support Vector Machine (SVM) classiﬁers. The classiﬁer k-NN, as a very simple classiﬁer can reveal the improvement in representational power of the resulting vectors in a proper manner. The SVM classiﬁer is applied as one of the most ﬂexible and powerful known classiﬁers in improving the classiﬁcation accuracy. \n In all experiments, three levels including: original graphs, the ﬁrst and the second abstract level graphs are embedded ([math]). With respect to the applications in this study, the higher level summaries are similar and it is observed that the ﬁrst three levels convey appropriate information for classiﬁcation. The reduction factor of the summarization algorithm is [math], inspired from the regular image pyramid. For the topological graphs of the underlying application, the proposed summarization algorithm is used. The afﬁnity matrix is calculated by Gaussian kernel function, where [math] if [math] and 0 otherwise. The scaling parameter [math] is considered as the standard deviation of distances among all pairs of node labels. The parameter [math] depends on the structure of the underlying graphs. Because in these applications the graphs are sparse, i.e., they contain relatively few edges, a small value ([math]) of the [math] is selected here, allowing for insertion of one super-edge between any two super-nodes of a summary graph even if there is an edge between any two nodes. Here, [math] is set to 0.5, indicating that if the number of edges in a super-node is at least equal to half the number of edges of a complete graph, a loop is inserted on the super-node."
    },
    "19": {
      "id": "5.2.1. Performance evaluation of summary graphs",
      "content": "The objective of graph summarization algorithm is to preserve the main structures of original graphs as much as possible. To have a better perspective of the representational power, the extracted graphs from the images of Gaussian pyramid are compared with summary graphs obtained from this summarization algorithm in Fig. 4. It is observed that there exists a similarity between the summary graphs discovery of global structures and graphs of the reduced resolution images. The witness of this claim is the similar structures of the corresponding graphs. \n To study this effect more precisely, a k-NN classiﬁer is assessed here based on edit distances in graph domain. Regarding the nu-merical nature of node labels, the Euclidean cost function [1] is applied in order to compute the edit distance. The cost for edit operations is set through a validation procedure similar to [1]. However, some heuristics for estimating the initial intervals of costs are applied. As is observed in Table 2, the summarization algorithm has the considerable ability of preserving the required information for classiﬁcation. It is conﬁrmed by the close and sometimes equal classiﬁcation accuracies of the summary and original graphs. These results open up the possibility of using the summaries instead of their larger counterparts to avoid time-consuming graph edit distance computations."
    },
    "20": {
      "id": "5.2.2. Pyramidal graph embedding on single-scale methods‌",
      "content": "Here, the results of the pyramidal graph embedding application on three selected methods are presented. In all experiments here, the [math] indicates the set of all possible combinations of the three levels applied in the graph‌ pyramid. At each [math] combination, [math], if the ith abstract level is available in the ﬁnal feature vector, and [math] otherwise. Obviously, [math] is related to the selected single-scale embedding method."
    },
    "21": {
      "id": "5.2.3. PyrDGE method",
      "content": "The dissimilarity-based Graph Embedding (DGE) method introduced by Riesen and Bunke [1] is usually adopted as a reference method in comparing with other embedding methods. It is of high accuracy and of course high time complexity as well and can be applied on all kinds of graphs. Here, the Spanning Prototype Selector (SPS) is applied as the prototype selection strategy for this method [39]. \n In PyrDGE experiments, DGE is applied on all combinations of the pyramidal framework and the classiﬁcation accuracies of k-NN ([math]) and SVM are reported in Table 3. In k-NN test, the number of prototypes of each available level should be tuned for every combination. This tuning is made by varying these parameters in the certain intervals and searching for the best conﬁguration regarding the classiﬁcation accuracy on the validation set simultaneously. In SVM test, for each level of combination, ﬁxing the best performing number of prototypes obtained in k-NN test, the SVM meta-parameters are validated in a separate step. Although this operation yields the sub-optimal results, it is accepted in this study, since the joint task is computationally infeasible. The best validated linear kernel SVM and the best validated RBF kernel SVM are applied on the test set separately and the maximum of the accuracies is reported. In most cases, the maximum accuracy is obtained through RBF kernel, therefore this kernel is suggested for SVM classiﬁcation. \n It is observed that the accuracy improves when the base level is augmented with even a single summary level. This fact implies that the global information extracted from the summary levels can compensate the information loss occurred during the embedding of the original graphs to some extent. \n The resulting accuracies of [math] combination are superior in comparison with the base level embedding in almost half of the cases. This property is observed in assessing other test embedding methods more or less. Here it is deduced that in these cases, the global information is of substantial importance in classiﬁcation process. In some methods like DGE where the computation time of edit distance between graphs increases dramatically with an increase in graph order, applying two summary levels instead of the base level can make a tradeoff between accuracy and speed. \n To have a better comparison between the accuracies of k-NN applied on graph domain and the obtained vectors of DGE and PyrDGE, the best graph domain accuracies from Table 2 are replicated into Table 3. A surprising point worth noting here is that the best PyrDGE accuracy is close to the graph domain classiﬁcation accuracy, even higher in some cases. This phenomenon indicates that different aspects of the graph structure extracted by the pyramidal embedding contribute to the preservation of the superior representational power of graphs of the application. Although the loss of some graph information is a built-in characteristic of the graph embedding approach, putting the vectors of different levels together can better reconstruct the graph features than to merely taking advantage from the original graphs."
    },
    "22": {
      "id": "5.2.4. PyrNRS method",
      "content": "The selected method from the probing based group is the hard version of Node Representative Statistics (NRS) method [2,40]. This method can process attributed-node graphs in the linear time complexity due to just local information usage. In the experiments of NRS and this newly developed PyrNRS, a validation phase for the meta-parameters of k-NN and SVM classiﬁers is run in a  manner similar to that of the previous experiment, except that the set of representatives should be selected instead of the prototypes. The representatives are collected through a simple spanning method on the set of all node labels in the training graphs. The results obtained from all combinations of PyrNRS are tabulated in Table 4. \n NRS method takes the graph content and the local information into account by counting the node representatives and the binary relations among them, respectively. Hence, there is a high possibility that two graphs with completely different global structures but similar label content are mapped into the close vectors. The node label relations in different abstract levels of two graphs are compared through PyrNRS and the possibility of this effect is reduced. This claim is justiﬁed by the improved precision reported in Table 4. \n The authors of [2] suggested the use of DGE in spite of its high computational complexity in cases that the NRS method is of the low classiﬁcation rates. The results obtained here indicate that PyrNRS can be considered as an appropriate alternative for DGE in such cases. The PyrNRS accuracy is noticeable against DGE (e.g. 64% of PyrNRS in comparison with 62.67% of DGE on SVM classiﬁcation of ODBK graphs), while the computation time is lower than that of the DGE."
    },
    "23": {
      "id": "5.2.5. PyrIZF method",
      "content": "The spectral methods are appropriate options in processing the unlabeled graphs which afford an opportunity for assessing the power of pyramidal framework in enhancing the capability of feature vectors in graph structure representation. Ihara Zeta Function (IZF) method [5] applies the reciprocal of the Zeta function, [math], where T is the adjacency matrix of the oriented line graph and I is the identity matrix. The [math] can be represented by a linear combination of the powers of u as [math]. The embedded vector in this method is computed through the limited number of coefﬁcients [math] because the higher order coefﬁcients contain redundant information. The IZF method collects the global information by considering the number of prime cycles with different lengths, and can be regarded as an embedding method with the global nature. \n Our initial experiments indicate that decreasing the classiﬁcation accuracy by an increase in the number of classes in IZF is more than DGE and NRS. Moreover, the similarity between graph structures of different classes reduces the accuracy signiﬁcantly. Hence, eight different classes from ODBK dataset with different node distributions exhibited in Table 5 are selected to study the behavior of the pyramidal framework. The graphs are assumed free of labels, in other words the node labels can be considered to have equal value. With this consideration, the calculated afﬁnity matrix by Gaussian kernel function is equated to the adjacency matrix. \n To assess PyrIZF, two distinct experiments with 10 and 15 coefﬁcients are run where each level of graph pyramid is embedded into the same length vector space. The classiﬁcation accuracies are computed similar to that of [6], where, 10-fold cross validation is applied and the averages of 10 runs are reported in Table 6 of this study. The results of k-NN applied on PyrIZF vectors indicate that the structural similarities and dissimilarities are more vivid in the second level of abstraction; however, this phenomenon cannot be predicted in advance. For example, while the accuracy of IZF for eight classes is 48.13%, the same for PyrIZF is 50.00% where only second layer of abstraction is applied. Additionally, ﬁndings of some other experiments imply that PyrIZF can decrease the sensitiveness of IZF method to the increasing in the number of classes and between class structural similarities."
    },
    "24": {
      "id": "5.3. Categorical attributed graphs",
      "content": "The objective here is to show how to adopt embedding and summarization methods for the categorical dataset. In this regard, similar setting of the pyramidal framework to Section 5.2.2 is used, but suggested summarization algorithm of Section 4.1.2 (k-SNAP) is considered to deal with this kind of graphs. \n The Enzymes dataset has only three distinct labels for a single node attribute. The NRS method, by considering these labels as representatives set, embeds each graph into a short vector based on the frequency of appearance of each representative and the six relations between them [40]. However, this simple representation is not informative enough for differentiating the complex graphs of this dataset. Therefore, NRS is not ﬁt for this application. The IZF method considers graphs without their labels and is not ﬁt for assessing graphs with categorical labels. Hence, the selected embedding method for this experiment is DGE due to its ability in dealing with graphs with different characteristics. The dirac function [1] is used for node substitution cost in computing graph edit distances. The precisions for k-NN classiﬁer with [math] and SVM classiﬁer are tabulated in Table 7. The results indicate that PyrDGE outperforms DGE in the cases that one or more abstract levels are added to the base level."
    },
    "25": {
      "id": "5.4. Memory requirement and runtime",
      "content": "It is obvious that the required amount of memory to store pyramid nodes is of [math]; however, the memory cost is mainly associated with the neighborhood relationships between the nodes; therefore, the maximum required memory in this case is [math] which can be decreased through the sparse representation of the graphs. \n In our implementation, the time complexities of both DGE and IZF methods are of [math] and the pyramidal framework does not increase them provided that the time complexity of the employed summarization technique is at most [math], which holds true for the proposed summarization algorithm. This statement is not true for NRS method which is of linear time order in the number of nodes and edges, however the extra time is bearable regarding to the remarkable increased accuracy. \n The running times for summarization and embedding of different levels of test graphs of ODBK and COIL datasets are presented in Fig. 5. These times are obtained on Intel Core I5 2.5 GHz machine running Windows 8 operating system. In order to validate comparison, a unique conﬁguration of meta-parameters is applied for embedding of all levels. For DGE method, 50 graphs are selected randomly from the training set and the different levels of these graphs are considered as the ﬁxed sets of prototypes. It is observed that there is a notable difference between the embedding time of the base level and two other levels, while the summarization time in all levels is negligible. In ODBK dataset, the embedding time in the third level of pyramid is almost 3.5% of that of the base level. For NRS method, a representative set of size 30 is selected randomly from the range of node labels for embedding each level. It is observed that the summarization procedure increases the overall embedding time of the second and third level in compare to the time of the base level, but these runtimes is still much less than that of DGE method. For IZF method, 15 Ihara coefﬁcients are utilized for embedding of each level. The runtime is similar to that of PyrNRS and the same discussion can be presented."
    },
    "26": {
      "id": "6. Conclusions",
      "content": "One of the drawbacks of graph embedding paradigm is the reduction in the informative graph representation strength. To ease this drawback, a hierarchical framework through graph pyrmaid is proposed which extracts local and global features from different scales of given graph at the same time. In order to conﬁrm the ability of this framework, it is applied on three different embedding methods which have different perspectives on embedding. This framework is applied on the topological graphs of objects as well as the graphs of proteins and the results indicate that it can be beneﬁcial in different graph domains, provided that the appropriate graph embedding and graph summarization methods are selected. Some of the advantages of pyramidal framework in particular are: (I) ﬁnding models and substructures of lower levels through the low cost analysis of upper levels, (II) reducing the computational complexity through processing at higher levels of abstraction, (III) processing of different resolutions of graph based on application on an independent basis, (IV) validation of the importance of different abstract levels when the structure is not known in advance, and (V) bridging the gap between elementary and more descriptive features. \n The objective here is to exhibit the advantages of this proposed framework in terms of accuracy and time in a package. So, experiments are free of any pre-processing and post-processing operations. Feature selection methods, dimension reduction of merged vectors, weighting and rescaling vector components can be beneﬁcial in obtaining more discriminative and shorter vectors. Among all, an approach to learn the number of levels and the importance of each level followed by a weighting procedure will be advantageous. Additionally, different embedding methods can be applied in different graph levels according to the characteristics of the underlying graphs (i.e. the graph order distribution and the amount of noise). \n This pyramidal graph embedding can assist the analysis of large graphs like social networks. Of course a tradeoff between the accuracy and time should be established through a subtle assessment."
    }
  }
}
