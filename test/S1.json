{
  "metadata": {
    "title": "Distributed Representations of Sentences and Documents",
    "authors": "Quoc Le, Tomas Mikolov"
  },
  "tracks": {
    "1": {
      "id": "title and authors",
      "content": "Distributed Representations of Sentences and Documents by Quoc Le, Tomas Mikolov"
    },
    "2": {
      "id": "Abstract",
      "content": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant."
    },
    "3": {
      "id": "1. Introduction",
      "content": "Text classification and clustering play an important role in many applications, e.g, document retrieval, web search, spam filtering. At the heart of these applications is machine learning algorithms such as logistic regression or Kmeans."
    },
    "4": {
      "id": "2. Algorithms",
      "content": "We start by discussing previous methods for learning word vectors. These methods are the inspiration for our Paragraph Vector methods."
    },
    "5": {
      "id": "2.1 Learning Vector Representation of Words",
      "content": "This section introduces the concept of distributed vector representation of words. A well known framework for learning the word vectors is shown in Figure 1."
    },
    "6": {
      "id": "2.2. Paragraph Vector: A distributed memory model",
      "content": "Our approach for learning paragraph vectors is inspired by the methods for learning the word vectors. The inspiration is that the word vectors are asked to contribute to a prediction task about the next word in the sentence."
    },
    "7": {
      "id": "2.3. Paragraph Vector without word ordering: Distributed bag of words",
      "content": "The above method considers the concatenation of the paragraph vector with the word vectors to predict the next word in a text window."
    },
    "8": {
      "id": "3. Experiments",
      "content": "We perform experiments to better understand the behavior of the paragraph vectors."
    },
    "9": {
      "id": "3.1. Sentiment Analysis with the Stanford Sentiment Treebank Dataset",
      "content": "Dataset: This dataset was first proposed by (Pang & Lee, 2005) and subsequently extended by (Socher et al., 2013b) as a benchmark for sentiment analysis."
    },
    "10": {
      "id": "3.2. Beyond One Sentence: Sentiment Analysis with IMDB dataset",
      "content": "Some of the previous techniques only work on sentences, but not paragraphs/documents with several sentences. For instance,"
    },
    "11": {
      "id": "3.3. Information Retrieval with Paragraph Vectors",
      "content": "We turn our attention to an information retrieval task which requires fixed-length representations of paragraphs. \n Here, we have a dataset of paragraphs in the first 10 results returned by a search engine given each of 1,000,000 most popular queries."
    },
    "12": {
      "id": "3.4. Some further observations",
      "content": "We perform further experiments to understand various aspects of the models. Hereâ€™s some observations \n PV-DM is consistently better than PV-DBOW."
    },
    "13": {
      "id": "4. Related Work",
      "content": "Distributed representations for words were first proposed in (Rumelhart et al., 1986) and have become a successful paradigm, especially for statistical language modeling (Elman, 1990; Bengio et al., 2006;Mikolov, 2012)."
    },
    "14": {
      "id": "5. Discussion",
      "content": "We described Paragraph Vector, an unsupervised learning algorithm that learns vector representations for variablelength pieces of texts such as sentences and documents."
    },
  }
}
