{
    "1": {
        "content": "Convolutional neural networks  1. Spectral formulation. A spectral graph theoretical formulation of CNNs on graphs built on established tools in graph signal processing (GSP). ",
        "id": "1 Introduction"
    },
    "10": {
        "content": "In the sequel, we refer to the non-parametric and non-localized filters (2) as Non-Param, the filters (7) proposed in ",
        "id": "4 Numerical Experiments"
    },
    "11": {
        "content": "To validate our model, we applied it to the Euclidean case on the benchmark MNIST classification problem  where z i is the 2D coordinate of pixel i. This is an important sanity check for our model, which must be able to extract features on any graph, including the regular 2D grid. ",
        "id": "4.1 Revisiting Classical CNNs on MNIST"
    },
    "12": {
        "content": "",
        "id": "Accuracy"
    },
    "13": {
        "content": "Non-Param (2) Spline  learning rate of 0.03, learning rate decay of 0.95, momentum of 0.9. Filters are of size 5 \u00d7 5 and graph filters have the same support of K = 25. All models were trained for 20 epochs.",
        "id": "Dataset Architecture"
    },
    "14": {
        "content": "To demonstrate the versatility of our model to work with graphs generated from unstructured data, we applied our technique to the text categorization problem on the 20NEWS dataset which consists of 18,846 (11,314 for training and 7,532 for testing) text documents associated with 20 classes ",
        "id": "4.2 Text Categorization on 20NEWS"
    },
    "15": {
        "content": "",
        "id": "4.3 Comparison between Spectral Filters and Computational Efficiency"
    },
    "16": {
        "content": "For any graph CNN to be successful, the statistical assumptions of locality, stationarity, and compo- sitionality regarding the data must be fulfilled on the graph where the data resides. Therefore, the learned filters' quality and thus the classification performance critically depends on the quality of   the graph. For data lying on Euclidean space, experiments in Section 4.1 show that a simple k-NN graph of the grid is good enough to recover almost exactly the performance of standard CNNs. We also noticed that the value of k does not have a strong influence on the results. We can witness the importance of a graph satisfying the data assumptions by comparing its performance with a random graph. ",
        "id": "4.4 Influence of Graph Quality"
    },
    "17": {
        "content": "In this paper, we have introduced the mathematical and computational foundations of an efficient generalization of CNNs to graphs using tools from GSP. Experiments have shown the ability of the model to extract local and stationary features through graph convolutional layers. Compared with the first work on spectral graph CNNs introduced in ",
        "id": "5 Conclusion and Future Work"
    },
    "2": {
        "content": "Generalizing CNNs to graphs requires three fundamental steps: (i) the design of localized convolu- tional filters on graphs, (ii) a graph coarsening procedure that groups together similar vertices and (iii) a graph pooling operation that trades spatial resolution for higher filter resolution.",
        "id": "2 Proposed Technique"
    },
    "3": {
        "content": "There are two strategies to define convolutional filters; either from a spatial approach or from a spectral approach. By construction, spatial approaches provide filter localization via the finite size of the kernel. However, although graph convolution in the spatial domain is conceivable, it faces the challenge of matching local neighborhoods, as pointed out in  Graph Fourier Transform. We are interested in processing signals defined on undirected and connected graphs G = (V, E, W ), where V is a finite set of |V| = n vertices, E is a set of edges and W \u2208 R n\u00d7n is a weighted adjacency matrix encoding the connection weight between two vertices. A signal x : V \u2192 R defined on the nodes of the graph may be regarded as a vector x \u2208 R n where x i is the value of x at the i th node. An essential operator in spectral graph analysis is the graph Laplacian  where I n is the identity matrix. As L is a real symmetric positive semidefinite matrix, it has a complete set of orthonormal eigenvectors {u l } n\u22121 l=0 \u2208 R n , known as the graph Fourier modes, and their associated ordered real nonnegative eigenvalues {\u03bb l } n\u22121 l=0 , identified as the frequencies of the graph. The Laplacian is indeed diagonalized by the Fourier basis U = [u 0 , . . . , u n\u22121 ] \u2208 R n\u00d7n such that L = U \u039bU T where \u039b = diag([\u03bb 0 , . . . , \u03bb n\u22121 ]) \u2208 R n\u00d7n . The graph Fourier transform of a signal x \u2208 R n is then defined as\u02c6xas\u02c6 as\u02c6x = U T x \u2208 R n , and its inverse as x = U \u02c6 x  Spectral filtering of graph signals. As we cannot express a meaningful translation operator in the vertex domain, the convolution operator on graph * G is defined in the Fourier domain such that , where is the element-wise Hadamard product. It follows that a signal x is filtered by g \u03b8 as (1) A non-parametric filter, i.e. a filter whose parameters are all free, would be defined as where the parameter \u03b8 \u2208 R n is a vector of Fourier coefficients. Polynomial parametrization for localized filters. There are however two limitations with non- parametric filters: (i) they are not localized in space and (ii) their learning complexity is in O(n), the dimensionality of the data. These issues can be overcome with the use of a polynomial filter where the parameter \u03b8 \u2208 R K is a vector of polynomial coefficients. The value at vertex j of the filter g \u03b8 centered at vertex i is given by , where the kernel is localized via a convolution with a Kronecker delta function \u03b4 i \u2208 R n . By [12,  where d G is the shortest path distance, i.e. the minimum number of edges connecting two vertices on the graph. Consequently, spectral filters represented by K th - order polynomials of the Laplacian are exactly K-localized. Besides, their learning complexity is O(K), the support size of the filter, and thus the same complexity as classical CNNs. Recursive formulation for fast filtering. While we have shown how to learn localized filters with K parameters, the cost to filter a signal x as y = U g \u03b8 (\u039b)U T x is still high with O(n 2 ) op- erations because of the multiplication with the Fourier basis U . A solution to this problem is to parametrize g \u03b8 (L) as a polynomial function that can be computed recursively from L, as K mul- tiplications by a sparse L costs O(K|E|) O(n 2 ). One such polynomial, traditionally used in GSP to approximate kernels (like wavelets), is the Chebyshev expansion  . . , L K\u22121 x}, seems attractive because of the coefficients' independence. It is however more convoluted and thus left as a future work. Recall that the Chebyshev polynomial T k (x) of order k may be computed by the stable recurrence relation , the Hilbert space of square integrable functions with respect to the measure dy/ 1 \u2212 y 2 . A filter can thus be parametrized as the truncated expansion of order K \u2212 1, where the parameter \u03b8 \u2208 R K is a vector of Chebyshev coefficients and T k ( \u02dc \u039b) \u2208 R n\u00d7n is the Chebyshev polynomial of order k evaluated at\u02dc\u039bat\u02dc at\u02dc\u039b = 2\u039b/\u03bb max \u2212 I n , a diagonal matrix of scaled eigenvalues that lie in [\u22121, 1]. The filtering operation can then be written as Learning filters. The j th output feature map of the sample s is given by where the x s,i are the input feature maps and the F in \u00d7 F out vectors of Chebyshev coefficients \u03b8 i,j \u2208 R K are the layer's trainable parameters. When training multiple convolutional layers with the backpropagation algorithm, one needs the two gradients where E is the loss energy over a mini-batch of S samples. Each of the above three computations boils down to K sparse matrix-vector multiplications and one dense matrix-vector multiplication for a cost of O(K|E|F in F out S) operations. These can be efficiently computed on parallel architectures by leveraging tensor operations. Eventually, [\u00af x s,i,0 , . . . , \u00af x s,i,K\u22121 ] only needs to be computed once.",
        "id": "2.1 Learning Fast Localized Spectral Filters"
    },
    "4": {
        "content": "The pooling operation requires meaningful neighborhoods on graphs, where similar vertices are clustered together. Doing this for multiple layers is equivalent to a multi-scale clustering of the graph that preserves local geometric structures. It is however known that graph clustering is NP-hard  Graclus ",
        "id": "2.2 Graph Coarsening"
    },
    "5": {
        "content": "Pooling operations are carried out many times and must be efficient. After coarsening, the vertices of the input graph and its coarsened versions are not arranged in any meaningful way. Hence, a direct application of the pooling operation would need a table to store all matched vertices. That would result in a memory inefficient, slow, and hardly parallelizable implementation. It is however possible to arrange the vertices such that a graph pooling operation becomes as efficient as a 1D pooling. We proceed in two steps: (i) create a balanced binary tree and (ii) rearrange the vertices. After coarsening, each node has either two children, if it was matched at the finer level, or one, if it was not, i.e the node was a singleton. From the coarsest to finest level, fake nodes, i.e. disconnected nodes, are added to pair with the singletons such that each node has two children. This structure is a balanced binary tree: (i) regular nodes (and singletons) either have two regular nodes (e.g. level 1 vertex 0 in  analog to pooling a regular 1D signal. ",
        "id": "2.3 Fast Pooling of Graph Signals"
    },
    "6": {
        "content": "",
        "id": "3 Related Works"
    },
    "7": {
        "content": "The emerging field of GSP aims at bridging the gap between signal processing and spectral graph theory ",
        "id": "3.1 Graph Signal Processing"
    },
    "8": {
        "content": "The Graph Neural Network framework ",
        "id": "3.2 CNNs on Non-Euclidean Domains"
    },
    "9": {
        "content": "Architecture Accuracy Classical CNN C32-P4-C64-P4-FC512 99.33 Proposed graph CNN GC32-P4-GC64-P4-FC512 99.14  tion on mesh patches, and formulated a deep learning architecture which allows comparison across different manifolds. They obtained state-of-the-art results for 3D shape recognition. The first spectral formulation of a graph CNN, proposed in  where B \u2208 R n\u00d7K is the cubic B-spline basis and the parameter \u03b8 \u2208 R K is a vector of control points. They later proposed a strategy to learn the graph structure from the data and applied the model to image recognition, text categorization and bioinformatics ",
        "id": "Model"
    }
}
{
    "authors": "Micha\u00ebl Defferrard, Xavier Bresson, Pierre Vandergheynst",
    "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering"
}
